{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Experiments\n",
    "\n",
    "1. try adding more dense layers\n",
    "2. play around with learning rate and epochs\n",
    "3. Using for loop when predicting\n",
    "4. Don't change the loss and metrics it's gonna give error\n",
    "5. Add embedding layer \n",
    "6. Add days information to the input\n",
    "7. Add mask\n",
    "8. Change Input and Output Sequence length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('customer_wise_aggregated_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data for sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_movies_20 = np.array([seq[:20] for seq in data['movie_ids'] if len(seq) > 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = keras.preprocessing.text.Tokenizer(split=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.fit_on_texts(data['movie_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_movies = movies.texts_to_sequences(data['movie_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_movies_with_threshold = np.array([seq[:20] for seq in tokenized_movies if len(seq) > 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254529, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_movies_with_threshold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_txt_movies = movies.sequences_to_texts(tokenized_movies_with_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254529"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_to_txt_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_tokenizer = keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_tokenizer.fit_on_texts(seq_to_txt_movies[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_tokenized = np.array(movies_tokenizer.texts_to_sequences(seq_to_txt_movies[:50000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['705 1267 1561 2095 2456 3423 723 30 157 173 329 457 1314 1428 1615 1693 2452 2782 2851 3290']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tokenizer.sequences_to_texts([movies_tokenized[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'705  1267  1561  2095  2456  3423  723  30  157  173  329  457  1314  1428  1615  1693  2452  2782  2851  3290'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_to_txt_movies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_vocab_size = len(movies_tokenizer.word_counts) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Target Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_X = movies_tokenized[:, :-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_y = movies_tokenized[:, -15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 132,   81,   87,   53,  359,  865,  267,   22, 1611,  804,   46,\n",
       "         28,  215,   11,   79])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 865,  267,   22, 1611,  804,   46,   28,  215,   11,   79,  447,\n",
       "          5,   15,  550,   84])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_tfdata(train_feat_dict, train_target_tensor, batch_size, buffer_size=None):\n",
    "    \"\"\"\n",
    "    Create train tf dataset for model train input\n",
    "    :param train_feat_dict: dict, containing the features tensors for train data\n",
    "    :param train_target_tensor: np.array(), the training TARGET tensor\n",
    "    :param batch_size: (int) size of the batch to work with\n",
    "    :param buffer_size: (int) Optional. Default is None. Size of the buffer\n",
    "    :return: (tuple) 1st element is the training dataset,\n",
    "                     2nd is the number of steps per epoch (based on batch size)\n",
    "    \"\"\"\n",
    "    if buffer_size is None:\n",
    "        buffer_size = batch_size*50\n",
    "\n",
    "    train_steps_per_epoch = len(train_target_tensor) // batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_feat_dict, train_target_tensor)).cache()\n",
    "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    train_dataset = train_dataset.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, train_steps_per_epoch\n",
    "  \n",
    "train_feat_dict = {'item_id': training_data_X} #{'item_id': train_dict['item_id'],\n",
    "                    # 'nb_days': train_dict['nb_days']}\n",
    "train_target_tensor = training_data_y #train_dict['target']\n",
    "\n",
    "train_dataset, train_steps_per_epoch = create_train_tfdata(train_feat_dict, train_target_tensor, batch_size=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    We redefine our own loss function in order to get rid of the '0' value\n",
    "    which is the one used for padding. This to avoid that the model optimize itself\n",
    "    by predicting this value because it is the padding one.\n",
    "    \n",
    "    :param real: the truth\n",
    "    :param pred: predictions\n",
    "    :return: a masked loss where '0' in real (due to padding)\n",
    "                are not taken into account for the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # to check that pred is numric and not nan\n",
    "    # mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss_ = loss_object_(real, pred)\n",
    "    # mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, max_len, item_vocab_size):\n",
    "    \"\"\"\n",
    "    Build a model given the hyper-parameters with item and nb_days input features\n",
    "    :param hp: (kt.HyperParameters) hyper-parameters to use when building this model\n",
    "    :return: built and compiled tensorflow model \n",
    "    \"\"\"\n",
    "    # inputs = {}\n",
    "    inputs = tf.keras.Input(batch_input_shape=[None, 15],\n",
    "                                       name='item_id', dtype=tf.int32)\n",
    "    # create encoding padding mask\n",
    "    # encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['item_id'], 0))\n",
    "\n",
    "    # nb_days bucketized\n",
    "    # inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "    #                                    name='nb_days', dtype=tf.int32)\n",
    "\n",
    "    # Pass categorical input through embedding layer\n",
    "    # with size equals to tokenizer vocabulary size\n",
    "    # Remember that vocab_size is len of item tokenizer + 1\n",
    "    # (for the padding '0' value)\n",
    "    \n",
    "    embedding_item = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
    "                                               output_dim=hp.get('embedding_item'),\n",
    "                                               name='embedding_item'\n",
    "                                              )(inputs)\n",
    "    # nbins=100, +1 for zero padding\n",
    "    # embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\n",
    "    #                                               output_dim=hp.get('embedding_nb_days'),\n",
    "    #                                               name='embedding_nb_days'\n",
    "    #                                              )(inputs['nb_days'])\n",
    "\n",
    "    #  Concatenate embedding layers\n",
    "    # concat_embedding_input = tf.keras.layers.Concatenate(\n",
    "    #  name='concat_embedding_input')([embedding_item, embedding_nb_days])\n",
    "\n",
    "    # concat_embedding_input\n",
    "    batchnorm_inputs = tf.keras.layers.BatchNormalization(name='batchnorm_inputs')(embedding_item)\n",
    "    \n",
    "    # LSTM layer\n",
    "    rnn = tf.keras.layers.LSTM(units=hp.get('rnn_units_cat'),\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=False,\n",
    "                                   recurrent_initializer='glorot_normal',\n",
    "                                   name='LSTM_cat'\n",
    "                                   )(batchnorm_inputs)\n",
    "\n",
    "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm')(rnn)\n",
    "\n",
    "    # Self attention so key=value in inputs\n",
    "    att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                    name='attention')(inputs=[rnn, rnn],)\n",
    "                                                    #   mask=[encoding_padding_mask,\n",
    "                                                    #         encoding_padding_mask])\n",
    "\n",
    "    # Last layer is a fully connected one\n",
    "    output = tf.keras.layers.Dense(item_vocab_size, name='dense_1')(att)\n",
    "    # output = tf.keras.layers.Dense(15 ,activation='softmax' ,name='output')(dense_1)\n",
    "    # dense_2 = tf.keras.layers.Dense(15, name='dense_2')(dense_1)\n",
    "    # output = tf.keras.layers.Dense(1, name='output')(dense_2)\n",
    "\n",
    "    # output = tf.keras.layers.Dense(item_vocab_size, name='output')(att)\n",
    "\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss=loss_function,\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {'embedding_item': 64, 'rnn_units_cat': 32, 'learning_rate': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hp, max_len=15, item_vocab_size=tokenizer_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_id (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_item (Embedding)     (None, 15, 64)       280576      ['item_id[0][0]']                \n",
      "                                                                                                  \n",
      " batchnorm_inputs (BatchNormali  (None, 15, 64)      256         ['embedding_item[0][0]']         \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " LSTM_cat (LSTM)                (None, 15, 32)       12416       ['batchnorm_inputs[0][0]']       \n",
      "                                                                                                  \n",
      " batchnorm_lstm (BatchNormaliza  (None, 15, 32)      128         ['LSTM_cat[0][0]']               \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 15, 32)       0           ['batchnorm_lstm[0][0]',         \n",
      "                                                                  'batchnorm_lstm[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15, 4384)     144672      ['attention[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 438,048\n",
      "Trainable params: 437,856\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, train_dataset, steps_per_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Fit the Keras model on the training dataset for a number of given epochs\n",
    "    :param model: tf model to be trained\n",
    "    :param train_dataset: (tf.data.Dataset object) the training dataset\n",
    "                          used to fit the model\n",
    "    :param steps_per_epoch: (int) Total number of steps (batches of samples) before \n",
    "                            declaring one epoch finished and starting the next epoch.\n",
    "    :param epochs: (int) the number of epochs for the fitting phase\n",
    "    :return: tuple (mirrored_model, history) with trained model and model history\n",
    "    \"\"\"\n",
    "    \n",
    "    # mirrored_strategy allows to use multi GPUs when available\n",
    "    mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "        tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "    \n",
    "    with mirrored_strategy.scope():\n",
    "        mirrored_model = model\n",
    "\n",
    "    history = mirrored_model.fit(train_dataset,\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 epochs=epochs, verbose=2)\n",
    "\n",
    "    return mirrored_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/1k/rxcvpl3j2czdhhs6sg9lsc4m0000gn/T/ipykernel_35685/407096462.py:14: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/1k/rxcvpl3j2czdhhs6sg9lsc4m0000gn/T/ipykernel_35685/407096462.py:14: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 04:29:39.049629: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-29 04:29:39.133308: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 8s - loss: 6.2404 - sparse_categorical_accuracy: 0.0263 - 8s/epoch - 82ms/step\n",
      "Epoch 2/5\n",
      "100/100 - 7s - loss: 5.6141 - sparse_categorical_accuracy: 0.0374 - 7s/epoch - 72ms/step\n",
      "Epoch 3/5\n",
      "100/100 - 7s - loss: 5.5137 - sparse_categorical_accuracy: 0.0403 - 7s/epoch - 72ms/step\n",
      "Epoch 4/5\n",
      "100/100 - 7s - loss: 5.4557 - sparse_categorical_accuracy: 0.0419 - 7s/epoch - 73ms/step\n",
      "Epoch 5/5\n",
      "100/100 - 7s - loss: 5.4140 - sparse_categorical_accuracy: 0.0432 - 7s/epoch - 72ms/step\n"
     ]
    }
   ],
   "source": [
    "trained_model, training_history = fit_model(model, train_dataset, steps_per_epoch=100, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {'item_id': [training_data_X[0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat_dict = {'item_id': training_data_X[:5]} #{'item_id': train_dict['item_id'],\n",
    "                    # 'nb_days': train_dict['nb_days']}\n",
    "test_target_tensor = training_data_y[:5] #train_dict['target']\n",
    "\n",
    "test_dataset, _ = create_train_tfdata(train_feat_dict, train_target_tensor, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 231ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 04:30:23.323028: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-01-29 04:30:23.375279: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "predictions = trained_model.predict(test_dataset, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_id (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_item (Embedding)     (None, 15, 64)       280576      ['item_id[0][0]']                \n",
      "                                                                                                  \n",
      " batchnorm_inputs (BatchNormali  (None, 15, 64)      256         ['embedding_item[0][0]']         \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " LSTM_cat (LSTM)                (None, 15, 32)       12416       ['batchnorm_inputs[0][0]']       \n",
      "                                                                                                  \n",
      " batchnorm_lstm (BatchNormaliza  (None, 15, 32)      128         ['LSTM_cat[0][0]']               \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 15, 32)       0           ['batchnorm_lstm[0][0]',         \n",
      "                                                                  'batchnorm_lstm[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15, 4384)     144672      ['attention[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 438,048\n",
      "Trainable params: 437,856\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4384)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 132,   81,   87,   53,  359,  865,  267,   22, 1611,  804,   46,\n",
       "         28,  215,   11,   79])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 865,  267,   22, 1611,  804,   46,   28,  215,   11,   79,  447,\n",
       "          5,   15,  550,   84])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(tf.random.categorical(predictions[0], 15)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1394 187 75 758 3756 2475 1145 2430 1145 3282 758 2495 1571 2617 2612']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tokenizer.sequences_to_texts([t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - I - G - N - O - R - E -      Code after this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.396628 ,  4.414433 ,  3.3817878,  4.147898 ,  3.6213093,\n",
       "        3.5384274,  4.476161 ,  3.2653542,  3.4214551,  3.9680667,\n",
       "        3.12084  ,  2.9775157,  2.6365614,  4.1158466,  3.3386211,\n",
       "        3.1838474,  2.0544128,  1.5780928,  2.6273649,  4.2736964],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031356186"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]),)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(predictions[0][1] == 0.031356186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'788'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tokenizer.index_word[54]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1542 788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2472']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tokenizer.sequences_to_texts([[1542]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2472 907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'571': 1,\n",
       " '1905': 2,\n",
       " '1145': 3,\n",
       " '2152': 4,\n",
       " '2452': 5,\n",
       " '1542': 6,\n",
       " '483': 7,\n",
       " '3106': 8,\n",
       " '1798': 9,\n",
       " '1307': 10,\n",
       " '1428': 11,\n",
       " '175': 12,\n",
       " '985': 13,\n",
       " '607': 14,\n",
       " '2782': 15,\n",
       " '313': 16,\n",
       " '191': 17,\n",
       " '3427': 18,\n",
       " '2122': 19,\n",
       " '1962': 20,\n",
       " '1220': 21,\n",
       " '30': 22,\n",
       " '1180': 23,\n",
       " '2862': 24,\n",
       " '4306': 25,\n",
       " '3860': 26,\n",
       " '4432': 27,\n",
       " '457': 28,\n",
       " '708': 29,\n",
       " '798': 30,\n",
       " '2612': 31,\n",
       " '1470': 32,\n",
       " '1110': 33,\n",
       " '4123': 34,\n",
       " '3624': 35,\n",
       " '3962': 36,\n",
       " '2200': 37,\n",
       " '1202': 38,\n",
       " '3825': 39,\n",
       " '1144': 40,\n",
       " '758': 41,\n",
       " '312': 42,\n",
       " '1102': 43,\n",
       " '299': 44,\n",
       " '3938': 45,\n",
       " '329': 46,\n",
       " '2372': 47,\n",
       " '4356': 48,\n",
       " '1744': 49,\n",
       " '1406': 50,\n",
       " '3756': 51,\n",
       " '197': 52,\n",
       " '2095': 53,\n",
       " '788': 54,\n",
       " '2112': 55,\n",
       " '3925': 56,\n",
       " '1865': 57,\n",
       " '3368': 58,\n",
       " '357': 59,\n",
       " '2391': 60,\n",
       " '482': 61,\n",
       " '3638': 62,\n",
       " '2580': 63,\n",
       " '1799': 64,\n",
       " '4345': 65,\n",
       " '4043': 66,\n",
       " '1975': 67,\n",
       " '3371': 68,\n",
       " '886': 69,\n",
       " '3320': 70,\n",
       " '2800': 71,\n",
       " '2660': 72,\n",
       " '1650': 73,\n",
       " '3151': 74,\n",
       " '2342': 75,\n",
       " '1810': 76,\n",
       " '241': 77,\n",
       " '4472': 78,\n",
       " '1615': 79,\n",
       " '468': 80,\n",
       " '1267': 81,\n",
       " '3610': 82,\n",
       " '2617': 83,\n",
       " '3290': 84,\n",
       " '331': 85,\n",
       " '3905': 86,\n",
       " '1561': 87,\n",
       " '2743': 88,\n",
       " '2874': 89,\n",
       " '3254': 90,\n",
       " '2465': 91,\n",
       " '3282': 92,\n",
       " '2186': 93,\n",
       " '963': 94,\n",
       " '28': 95,\n",
       " '199': 96,\n",
       " '1754': 97,\n",
       " '2913': 98,\n",
       " '1703': 99,\n",
       " '2209': 100,\n",
       " '1843': 101,\n",
       " '3605': 102,\n",
       " '1590': 103,\n",
       " '1466': 104,\n",
       " '3917': 105,\n",
       " '4266': 106,\n",
       " '443': 107,\n",
       " '3079': 108,\n",
       " '270': 109,\n",
       " '3538': 110,\n",
       " '3333': 111,\n",
       " '872': 112,\n",
       " '550': 113,\n",
       " '1046': 114,\n",
       " '2578': 115,\n",
       " '658': 116,\n",
       " '1509': 117,\n",
       " '2699': 118,\n",
       " '143': 119,\n",
       " '551': 120,\n",
       " '1289': 121,\n",
       " '413': 122,\n",
       " '1324': 123,\n",
       " '3385': 124,\n",
       " '2395': 125,\n",
       " '3433': 126,\n",
       " '1073': 127,\n",
       " '692': 128,\n",
       " '2178': 129,\n",
       " '2015': 130,\n",
       " '4056': 131,\n",
       " '705': 132,\n",
       " '2953': 133,\n",
       " '1659': 134,\n",
       " '2499': 135,\n",
       " '295': 136,\n",
       " '473': 137,\n",
       " '2009': 138,\n",
       " '3446': 139,\n",
       " '2128': 140,\n",
       " '2992': 141,\n",
       " '1305': 142,\n",
       " '1255': 143,\n",
       " '290': 144,\n",
       " '4227': 145,\n",
       " '2290': 146,\n",
       " '1770': 147,\n",
       " '3936': 148,\n",
       " '2495': 149,\n",
       " '3148': 150,\n",
       " '252': 151,\n",
       " '896': 152,\n",
       " '1719': 153,\n",
       " '720': 154,\n",
       " '1571': 155,\n",
       " '3798': 156,\n",
       " '789': 157,\n",
       " '1625': 158,\n",
       " '4302': 159,\n",
       " '111': 160,\n",
       " '1832': 161,\n",
       " '1174': 162,\n",
       " '2171': 163,\n",
       " '4389': 164,\n",
       " '1877': 165,\n",
       " '2192': 166,\n",
       " '257': 167,\n",
       " '1700': 168,\n",
       " '3522': 169,\n",
       " '2360': 170,\n",
       " '1367': 171,\n",
       " '3579': 172,\n",
       " '494': 173,\n",
       " '1645': 174,\n",
       " '2430': 175,\n",
       " '4080': 176,\n",
       " '148': 177,\n",
       " '1861': 178,\n",
       " '1642': 179,\n",
       " '3626': 180,\n",
       " '2594': 181,\n",
       " '3315': 182,\n",
       " '4330': 183,\n",
       " '3466': 184,\n",
       " '4141': 185,\n",
       " '442': 186,\n",
       " '746': 187,\n",
       " '4216': 188,\n",
       " '2016': 189,\n",
       " '2457': 190,\n",
       " '3418': 191,\n",
       " '4488': 192,\n",
       " '4159': 193,\n",
       " '3713': 194,\n",
       " '2400': 195,\n",
       " '223': 196,\n",
       " '3463': 197,\n",
       " '406': 198,\n",
       " '2462': 199,\n",
       " '2000': 200,\n",
       " '83': 201,\n",
       " '2001': 202,\n",
       " '660': 203,\n",
       " '1066': 204,\n",
       " '2922': 205,\n",
       " '962': 206,\n",
       " '2470': 207,\n",
       " '1027': 208,\n",
       " '330': 209,\n",
       " '311': 210,\n",
       " '3684': 211,\n",
       " '862': 212,\n",
       " '4262': 213,\n",
       " '516': 214,\n",
       " '1314': 215,\n",
       " '1682': 216,\n",
       " '1833': 217,\n",
       " '3256': 218,\n",
       " '3966': 219,\n",
       " '1974': 220,\n",
       " '2960': 221,\n",
       " '273': 222,\n",
       " '3730': 223,\n",
       " '1138': 224,\n",
       " '2675': 225,\n",
       " '2443': 226,\n",
       " '2554': 227,\n",
       " '187': 228,\n",
       " '269': 229,\n",
       " '2680': 230,\n",
       " '353': 231,\n",
       " '2012': 232,\n",
       " '3824': 233,\n",
       " '1637': 234,\n",
       " '2779': 235,\n",
       " '851': 236,\n",
       " '361': 237,\n",
       " '1585': 238,\n",
       " '2690': 239,\n",
       " '3611': 240,\n",
       " '3138': 241,\n",
       " '4393': 242,\n",
       " '3782': 243,\n",
       " '3900': 244,\n",
       " '118': 245,\n",
       " '3098': 246,\n",
       " '2254': 247,\n",
       " '1918': 248,\n",
       " '1595': 249,\n",
       " '759': 250,\n",
       " '77': 251,\n",
       " '2905': 252,\n",
       " '2734': 253,\n",
       " '424': 254,\n",
       " '4384': 255,\n",
       " '2866': 256,\n",
       " '4315': 257,\n",
       " '3197': 258,\n",
       " '831': 259,\n",
       " '97': 260,\n",
       " '4364': 261,\n",
       " '334': 262,\n",
       " '1148': 263,\n",
       " '2252': 264,\n",
       " '940': 265,\n",
       " '937': 266,\n",
       " '723': 267,\n",
       " '4402': 268,\n",
       " '996': 269,\n",
       " '3350': 270,\n",
       " '4149': 271,\n",
       " '4135': 272,\n",
       " '501': 273,\n",
       " '1020': 274,\n",
       " '2938': 275,\n",
       " '3890': 276,\n",
       " '367': 277,\n",
       " '3347': 278,\n",
       " '1435': 279,\n",
       " '3168': 280,\n",
       " '256': 281,\n",
       " '1902': 282,\n",
       " '2072': 283,\n",
       " '563': 284,\n",
       " '615': 285,\n",
       " '2371': 286,\n",
       " '3161': 287,\n",
       " '4260': 288,\n",
       " '1482': 289,\n",
       " '811': 290,\n",
       " '1035': 291,\n",
       " '919': 292,\n",
       " '819': 293,\n",
       " '2780': 294,\n",
       " '348': 295,\n",
       " '1495': 296,\n",
       " '3875': 297,\n",
       " '1467': 298,\n",
       " '189': 299,\n",
       " '4269': 300,\n",
       " '3275': 301,\n",
       " '2153': 302,\n",
       " '108': 303,\n",
       " '1741': 304,\n",
       " '2163': 305,\n",
       " '1884': 306,\n",
       " '3954': 307,\n",
       " '1983': 308,\n",
       " '4341': 309,\n",
       " '3017': 310,\n",
       " '3355': 311,\n",
       " '686': 312,\n",
       " '1394': 313,\n",
       " '3078': 314,\n",
       " '1578': 315,\n",
       " '3787': 316,\n",
       " '385': 317,\n",
       " '46': 318,\n",
       " '3926': 319,\n",
       " '275': 320,\n",
       " '621': 321,\n",
       " '3198': 322,\n",
       " '4256': 323,\n",
       " '3085': 324,\n",
       " '2172': 325,\n",
       " '3312': 326,\n",
       " '818': 327,\n",
       " '4109': 328,\n",
       " '4210': 329,\n",
       " '3434': 330,\n",
       " '2162': 331,\n",
       " '636': 332,\n",
       " '989': 333,\n",
       " '285': 334,\n",
       " '2389': 335,\n",
       " '1050': 336,\n",
       " '1890': 337,\n",
       " '3153': 338,\n",
       " '216': 339,\n",
       " '156': 340,\n",
       " '2890': 341,\n",
       " '3274': 342,\n",
       " '1901': 343,\n",
       " '2942': 344,\n",
       " '3650': 345,\n",
       " '900': 346,\n",
       " '2329': 347,\n",
       " '3742': 348,\n",
       " '843': 349,\n",
       " '1256': 350,\n",
       " '1264': 351,\n",
       " '2136': 352,\n",
       " '4392': 353,\n",
       " '2803': 354,\n",
       " '3541': 355,\n",
       " '3894': 356,\n",
       " '3239': 357,\n",
       " '18': 358,\n",
       " '2456': 359,\n",
       " '1746': 360,\n",
       " '1408': 361,\n",
       " '1175': 362,\n",
       " '1425': 363,\n",
       " '58': 364,\n",
       " '994': 365,\n",
       " '2139': 366,\n",
       " '3817': 367,\n",
       " '829': 368,\n",
       " '3015': 369,\n",
       " '3489': 370,\n",
       " '1955': 371,\n",
       " '2965': 372,\n",
       " '4369': 373,\n",
       " '1370': 374,\n",
       " '3879': 375,\n",
       " '3267': 376,\n",
       " '4420': 377,\n",
       " '3253': 378,\n",
       " '3364': 379,\n",
       " '1707': 380,\n",
       " '711': 381,\n",
       " '1295': 382,\n",
       " '393': 383,\n",
       " '3071': 384,\n",
       " '4479': 385,\n",
       " '689': 386,\n",
       " '2775': 387,\n",
       " '670': 388,\n",
       " '1094': 389,\n",
       " '1854': 390,\n",
       " '3113': 391,\n",
       " '2078': 392,\n",
       " '4271': 393,\n",
       " '2955': 394,\n",
       " '44': 395,\n",
       " '416': 396,\n",
       " '1599': 397,\n",
       " '2574': 398,\n",
       " '3414': 399,\n",
       " '662': 400,\n",
       " '3326': 401,\n",
       " '2848': 402,\n",
       " '2376': 403,\n",
       " '2348': 404,\n",
       " '3648': 405,\n",
       " '564': 406,\n",
       " '2577': 407,\n",
       " '751': 408,\n",
       " '422': 409,\n",
       " '2783': 410,\n",
       " '954': 411,\n",
       " '8': 412,\n",
       " '4100': 413,\n",
       " '305': 414,\n",
       " '763': 415,\n",
       " '152': 416,\n",
       " '433': 417,\n",
       " '2478': 418,\n",
       " '4331': 419,\n",
       " '166': 420,\n",
       " '3309': 421,\n",
       " '1096': 422,\n",
       " '1709': 423,\n",
       " '2640': 424,\n",
       " '599': 425,\n",
       " '3058': 426,\n",
       " '3826': 427,\n",
       " '1479': 428,\n",
       " '859': 429,\n",
       " '1176': 430,\n",
       " '2558': 431,\n",
       " '2585': 432,\n",
       " '390': 433,\n",
       " '4012': 434,\n",
       " '3999': 435,\n",
       " '1632': 436,\n",
       " '4171': 437,\n",
       " '534': 438,\n",
       " '405': 439,\n",
       " '734': 440,\n",
       " '378': 441,\n",
       " '752': 442,\n",
       " '2149': 443,\n",
       " '459': 444,\n",
       " '2174': 445,\n",
       " '1441': 446,\n",
       " '1693': 447,\n",
       " '4287': 448,\n",
       " '3864': 449,\n",
       " '4418': 450,\n",
       " '4155': 451,\n",
       " '1862': 452,\n",
       " '2212': 453,\n",
       " '674': 454,\n",
       " '2385': 455,\n",
       " '2182': 456,\n",
       " '3165': 457,\n",
       " '3942': 458,\n",
       " '2532': 459,\n",
       " '33': 460,\n",
       " '2135': 461,\n",
       " '517': 462,\n",
       " '1532': 463,\n",
       " '524': 464,\n",
       " '4490': 465,\n",
       " '918': 466,\n",
       " '548': 467,\n",
       " '2173': 468,\n",
       " '629': 469,\n",
       " '3506': 470,\n",
       " '3893': 471,\n",
       " '1104': 472,\n",
       " '4127': 473,\n",
       " '4429': 474,\n",
       " '381': 475,\n",
       " '78': 476,\n",
       " '3535': 477,\n",
       " '1068': 478,\n",
       " '2362': 479,\n",
       " '2441': 480,\n",
       " '1530': 481,\n",
       " '248': 482,\n",
       " '677': 483,\n",
       " '2319': 484,\n",
       " '1012': 485,\n",
       " '138': 486,\n",
       " '3224': 487,\n",
       " '171': 488,\n",
       " '1969': 489,\n",
       " '953': 490,\n",
       " '1075': 491,\n",
       " '1708': 492,\n",
       " '1582': 493,\n",
       " '3680': 494,\n",
       " '1661': 495,\n",
       " '499': 496,\n",
       " '833': 497,\n",
       " '1163': 498,\n",
       " '426': 499,\n",
       " '2167': 500,\n",
       " '3174': 501,\n",
       " '2813': 502,\n",
       " '4355': 503,\n",
       " '3870': 504,\n",
       " '1610': 505,\n",
       " '580': 506,\n",
       " '1692': 507,\n",
       " '2256': 508,\n",
       " '2940': 509,\n",
       " '3046': 510,\n",
       " '3612': 511,\n",
       " '1792': 512,\n",
       " '554': 513,\n",
       " '3886': 514,\n",
       " '2519': 515,\n",
       " '2161': 516,\n",
       " '3510': 517,\n",
       " '1803': 518,\n",
       " '696': 519,\n",
       " '225': 520,\n",
       " '3216': 521,\n",
       " '445': 522,\n",
       " '3265': 523,\n",
       " '1602': 524,\n",
       " '4284': 525,\n",
       " '3521': 526,\n",
       " '1001': 527,\n",
       " '2548': 528,\n",
       " '3542': 529,\n",
       " '2314': 530,\n",
       " '2114': 531,\n",
       " '4339': 532,\n",
       " '3640': 533,\n",
       " '528': 534,\n",
       " '2423': 535,\n",
       " '4492': 536,\n",
       " '3128': 537,\n",
       " '2566': 538,\n",
       " '535': 539,\n",
       " '1673': 540,\n",
       " '3617': 541,\n",
       " '760': 542,\n",
       " '1224': 543,\n",
       " '3269': 544,\n",
       " '2284': 545,\n",
       " '52': 546,\n",
       " '4089': 547,\n",
       " '1100': 548,\n",
       " '668': 549,\n",
       " '2851': 550,\n",
       " '26': 551,\n",
       " '4496': 552,\n",
       " '3946': 553,\n",
       " '4353': 554,\n",
       " '3623': 555,\n",
       " '1558': 556,\n",
       " '3715': 557,\n",
       " '1395': 558,\n",
       " '1421': 559,\n",
       " '2326': 560,\n",
       " '682': 561,\n",
       " '3670': 562,\n",
       " '3160': 563,\n",
       " '3021': 564,\n",
       " '986': 565,\n",
       " '1735': 566,\n",
       " '3593': 567,\n",
       " '2809': 568,\n",
       " '533': 569,\n",
       " '283': 570,\n",
       " '2057': 571,\n",
       " '1721': 572,\n",
       " '1518': 573,\n",
       " '1553': 574,\n",
       " '3920': 575,\n",
       " '2751': 576,\n",
       " '213': 577,\n",
       " '2464': 578,\n",
       " '2482': 579,\n",
       " '2551': 580,\n",
       " '3728': 581,\n",
       " '1502': 582,\n",
       " '2705': 583,\n",
       " '1336': 584,\n",
       " '1011': 585,\n",
       " '2379': 586,\n",
       " '255': 587,\n",
       " '1971': 588,\n",
       " '1200': 589,\n",
       " '908': 590,\n",
       " '1743': 591,\n",
       " '2422': 592,\n",
       " '215': 593,\n",
       " '3544': 594,\n",
       " '1026': 595,\n",
       " '3182': 596,\n",
       " '1409': 597,\n",
       " '1151': 598,\n",
       " '3960': 599,\n",
       " '3439': 600,\n",
       " '1173': 601,\n",
       " '1297': 602,\n",
       " '1058': 603,\n",
       " '2386': 604,\n",
       " '2735': 605,\n",
       " '4138': 606,\n",
       " '1816': 607,\n",
       " '1939': 608,\n",
       " '1250': 609,\n",
       " '488': 610,\n",
       " '4157': 611,\n",
       " '1481': 612,\n",
       " '1098': 613,\n",
       " '2269': 614,\n",
       " '646': 615,\n",
       " '289': 616,\n",
       " '3030': 617,\n",
       " '253': 618,\n",
       " '431': 619,\n",
       " '894': 620,\n",
       " '2755': 621,\n",
       " '3758': 622,\n",
       " '209': 623,\n",
       " '2040': 624,\n",
       " '48': 625,\n",
       " '1819': 626,\n",
       " '2518': 627,\n",
       " '3571': 628,\n",
       " '1123': 629,\n",
       " '3170': 630,\n",
       " '420': 631,\n",
       " '240': 632,\n",
       " '2899': 633,\n",
       " '1976': 634,\n",
       " '3222': 635,\n",
       " '4248': 636,\n",
       " '569': 637,\n",
       " '167': 638,\n",
       " '1867': 639,\n",
       " '1665': 640,\n",
       " '1543': 641,\n",
       " '3736': 642,\n",
       " '1412': 643,\n",
       " '2643': 644,\n",
       " '438': 645,\n",
       " '232': 646,\n",
       " '4247': 647,\n",
       " '3935': 648,\n",
       " '2771': 649,\n",
       " '1618': 650,\n",
       " '1476': 651,\n",
       " '362': 652,\n",
       " '2916': 653,\n",
       " '722': 654,\n",
       " '79': 655,\n",
       " '3478': 656,\n",
       " '1925': 657,\n",
       " '4069': 658,\n",
       " '1757': 659,\n",
       " '1022': 660,\n",
       " '4031': 661,\n",
       " '1245': 662,\n",
       " '165': 663,\n",
       " '897': 664,\n",
       " '471': 665,\n",
       " '2216': 666,\n",
       " '2332': 667,\n",
       " '3223': 668,\n",
       " '127': 669,\n",
       " '1364': 670,\n",
       " '2659': 671,\n",
       " '1646': 672,\n",
       " '992': 673,\n",
       " '1552': 674,\n",
       " '3513': 675,\n",
       " '2103': 676,\n",
       " '638': 677,\n",
       " '4144': 678,\n",
       " '1594': 679,\n",
       " '825': 680,\n",
       " '1455': 681,\n",
       " '3469': 682,\n",
       " '2920': 683,\n",
       " '2876': 684,\n",
       " '4214': 685,\n",
       " '1633': 686,\n",
       " '3342': 687,\n",
       " '3703': 688,\n",
       " '3422': 689,\n",
       " '809': 690,\n",
       " '2043': 691,\n",
       " '643': 692,\n",
       " '1547': 693,\n",
       " '3551': 694,\n",
       " '749': 695,\n",
       " '84': 696,\n",
       " '3689': 697,\n",
       " '1092': 698,\n",
       " '2340': 699,\n",
       " '3285': 700,\n",
       " '1357': 701,\n",
       " '3379': 702,\n",
       " '359': 703,\n",
       " '3437': 704,\n",
       " '2045': 705,\n",
       " '268': 706,\n",
       " '2380': 707,\n",
       " '561': 708,\n",
       " '122': 709,\n",
       " '730': 710,\n",
       " '4197': 711,\n",
       " '17': 712,\n",
       " '57': 713,\n",
       " '1329': 714,\n",
       " '2251': 715,\n",
       " '2847': 716,\n",
       " '1627': 717,\n",
       " '700': 718,\n",
       " '345': 719,\n",
       " '4078': 720,\n",
       " '1790': 721,\n",
       " '1282': 722,\n",
       " '652': 723,\n",
       " '3567': 724,\n",
       " '47': 725,\n",
       " '1972': 726,\n",
       " '2228': 727,\n",
       " '398': 728,\n",
       " '2572': 729,\n",
       " '3054': 730,\n",
       " '680': 731,\n",
       " '3487': 732,\n",
       " '1044': 733,\n",
       " '840': 734,\n",
       " '1262': 735,\n",
       " '1300': 736,\n",
       " '1060': 737,\n",
       " '4396': 738,\n",
       " '1525': 739,\n",
       " '2033': 740,\n",
       " '1292': 741,\n",
       " '4493': 742,\n",
       " '661': 743,\n",
       " '3887': 744,\n",
       " '104': 745,\n",
       " '4237': 746,\n",
       " '3921': 747,\n",
       " '4360': 748,\n",
       " '2804': 749,\n",
       " '1800': 750,\n",
       " '4219': 751,\n",
       " '1689': 752,\n",
       " '3444': 753,\n",
       " '971': 754,\n",
       " '2189': 755,\n",
       " '3491': 756,\n",
       " '400': 757,\n",
       " '3220': 758,\n",
       " '1129': 759,\n",
       " '425': 760,\n",
       " '3124': 761,\n",
       " '242': 762,\n",
       " '180': 763,\n",
       " '3082': 764,\n",
       " '2889': 765,\n",
       " '2383': 766,\n",
       " '474': 767,\n",
       " '1804': 768,\n",
       " '725': 769,\n",
       " '2370': 770,\n",
       " '1399': 771,\n",
       " '1080': 772,\n",
       " '1860': 773,\n",
       " '432': 774,\n",
       " '2217': 775,\n",
       " '2460': 776,\n",
       " '3496': 777,\n",
       " '3740': 778,\n",
       " '4225': 779,\n",
       " '3858': 780,\n",
       " '2037': 781,\n",
       " '1130': 782,\n",
       " '977': 783,\n",
       " '3242': 784,\n",
       " '3500': 785,\n",
       " '2872': 786,\n",
       " '1160': 787,\n",
       " '907': 788,\n",
       " '2662': 789,\n",
       " '489': 790,\n",
       " '3523': 791,\n",
       " '3331': 792,\n",
       " '2350': 793,\n",
       " '208': 794,\n",
       " '2525': 795,\n",
       " '3608': 796,\n",
       " '2408': 797,\n",
       " '1126': 798,\n",
       " '2865': 799,\n",
       " '2701': 800,\n",
       " '1959': 801,\n",
       " '2528': 802,\n",
       " '3725': 803,\n",
       " '173': 804,\n",
       " '832': 805,\n",
       " '3814': 806,\n",
       " '3716': 807,\n",
       " '3903': 808,\n",
       " '1759': 809,\n",
       " '1299': 810,\n",
       " '1771': 811,\n",
       " '942': 812,\n",
       " '3840': 813,\n",
       " '2177': 814,\n",
       " '600': 815,\n",
       " '2512': 816,\n",
       " '360': 817,\n",
       " '2218': 818,\n",
       " '3573': 819,\n",
       " '3': 820,\n",
       " '4478': 821,\n",
       " '895': 822,\n",
       " '1837': 823,\n",
       " '2601': 824,\n",
       " '3835': 825,\n",
       " '961': 826,\n",
       " '1845': 827,\n",
       " '2533': 828,\n",
       " '2595': 829,\n",
       " '1047': 830,\n",
       " '3668': 831,\n",
       " '1142': 832,\n",
       " '1783': 833,\n",
       " '486': 834,\n",
       " '2590': 835,\n",
       " '1766': 836,\n",
       " '3505': 837,\n",
       " '2599': 838,\n",
       " '808': 839,\n",
       " '3404': 840,\n",
       " '4454': 841,\n",
       " '1913': 842,\n",
       " '990': 843,\n",
       " '1521': 844,\n",
       " '817': 845,\n",
       " '4390': 846,\n",
       " '3807': 847,\n",
       " '588': 848,\n",
       " '1226': 849,\n",
       " '1359': 850,\n",
       " '2176': 851,\n",
       " '45': 852,\n",
       " '4145': 853,\n",
       " '4255': 854,\n",
       " '427': 855,\n",
       " '1266': 856,\n",
       " '1704': 857,\n",
       " '3232': 858,\n",
       " '511': 859,\n",
       " '1668': 860,\n",
       " '1043': 861,\n",
       " '3013': 862,\n",
       " '3526': 863,\n",
       " '1392': 864,\n",
       " '3423': 865,\n",
       " '807': 866,\n",
       " '3934': 867,\n",
       " '3093': 868,\n",
       " '1848': 869,\n",
       " '1140': 870,\n",
       " '178': 871,\n",
       " '2475': 872,\n",
       " '2331': 873,\n",
       " '2467': 874,\n",
       " '3582': 875,\n",
       " '4380': 876,\n",
       " '2668': 877,\n",
       " '4450': 878,\n",
       " '1252': 879,\n",
       " '2366': 880,\n",
       " '304': 881,\n",
       " '586': 882,\n",
       " '3225': 883,\n",
       " '4282': 884,\n",
       " '659': 885,\n",
       " '2613': 886,\n",
       " '2102': 887,\n",
       " '201': 888,\n",
       " '2235': 889,\n",
       " '880': 890,\n",
       " '401': 891,\n",
       " '3862': 892,\n",
       " '2988': 893,\n",
       " '761': 894,\n",
       " '883': 895,\n",
       " '2302': 896,\n",
       " '1373': 897,\n",
       " '822': 898,\n",
       " '4463': 899,\n",
       " '4474': 900,\n",
       " '1607': 901,\n",
       " '281': 902,\n",
       " '3649': 903,\n",
       " '2356': 904,\n",
       " '3192': 905,\n",
       " '3301': 906,\n",
       " '3147': 907,\n",
       " '4049': 908,\n",
       " '358': 909,\n",
       " '3923': 910,\n",
       " '3743': 911,\n",
       " '1234': 912,\n",
       " '1471': 913,\n",
       " '4040': 914,\n",
       " '1103': 915,\n",
       " '3150': 916,\n",
       " '2954': 917,\n",
       " '1193': 918,\n",
       " '1111': 919,\n",
       " '456': 920,\n",
       " '16': 921,\n",
       " '2448': 922,\n",
       " '2262': 923,\n",
       " '3226': 924,\n",
       " '4352': 925,\n",
       " '978': 926,\n",
       " '4201': 927,\n",
       " '1963': 928,\n",
       " '408': 929,\n",
       " '850': 930,\n",
       " '55': 931,\n",
       " '1956': 932,\n",
       " '1947': 933,\n",
       " '71': 934,\n",
       " '582': 935,\n",
       " '4298': 936,\n",
       " '2974': 937,\n",
       " '1788': 938,\n",
       " '4253': 939,\n",
       " '3563': 940,\n",
       " '982': 941,\n",
       " '1844': 942,\n",
       " '1116': 943,\n",
       " '3410': 944,\n",
       " '1866': 945,\n",
       " '4072': 946,\n",
       " '672': 947,\n",
       " '133': 948,\n",
       " '2109': 949,\n",
       " '505': 950,\n",
       " '2900': 951,\n",
       " '1503': 952,\n",
       " '2868': 953,\n",
       " '3841': 954,\n",
       " '3810': 955,\n",
       " '2712': 956,\n",
       " '2113': 957,\n",
       " '645': 958,\n",
       " '3657': 959,\n",
       " '1375': 960,\n",
       " '1120': 961,\n",
       " '1931': 962,\n",
       " '436': 963,\n",
       " '238': 964,\n",
       " '1991': 965,\n",
       " '4147': 966,\n",
       " '3515': 967,\n",
       " '2023': 968,\n",
       " '2698': 969,\n",
       " '1999': 970,\n",
       " '2651': 971,\n",
       " '2711': 972,\n",
       " '2831': 973,\n",
       " '964': 974,\n",
       " '1401': 975,\n",
       " '3599': 976,\n",
       " '185': 977,\n",
       " '1087': 978,\n",
       " '3833': 979,\n",
       " '2104': 980,\n",
       " '4177': 981,\n",
       " '2912': 982,\n",
       " '344': 983,\n",
       " '4465': 984,\n",
       " '2140': 985,\n",
       " '4238': 986,\n",
       " '3453': 987,\n",
       " '724': 988,\n",
       " '1274': 989,\n",
       " '3880': 990,\n",
       " '3403': 991,\n",
       " '2861': 992,\n",
       " '3986': 993,\n",
       " '1384': 994,\n",
       " '3391': 995,\n",
       " '2757': 996,\n",
       " '3237': 997,\n",
       " '4106': 998,\n",
       " '956': 999,\n",
       " '3084': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tokenizer.word_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    }
   ],
   "source": [
    "trial_model = build_model(hp, max_len=15, item_vocab_size=tokenizer_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_id (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_item (Embedding)     (None, 15, 64)       280576      ['item_id[0][0]']                \n",
      "                                                                                                  \n",
      " batchnorm_inputs (BatchNormali  (None, 15, 64)      256         ['embedding_item[0][0]']         \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " LSTM_cat (LSTM)                (None, 15, 32)       12416       ['batchnorm_inputs[0][0]']       \n",
      "                                                                                                  \n",
      " batchnorm_lstm (BatchNormaliza  (None, 15, 32)      128         ['LSTM_cat[0][0]']               \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 15, 32)       0           ['batchnorm_lstm[0][0]',         \n",
      "                                                                  'batchnorm_lstm[0][0]']         \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 15, 1)        33          ['attention[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 293,409\n",
      "Trainable params: 293,217\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trial_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/metrics/metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 962, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 15 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast_1)' with input shapes: [?,15].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[346], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model, training_history \u001b[39m=\u001b[39m fit_model(trial_model, train_dataset, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[307], line 20\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model, train_dataset, steps_per_epoch, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m mirrored_strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m     18\u001b[0m     mirrored_model \u001b[39m=\u001b[39m model\n\u001b[0;32m---> 20\u001b[0m history \u001b[39m=\u001b[39m mirrored_model\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[1;32m     21\u001b[0m                              steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m     22\u001b[0m                              epochs\u001b[39m=\u001b[39;49mepochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m mirrored_model, history\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/1k/rxcvpl3j2czdhhs6sg9lsc4m0000gn/T/__autograph_generated_file31bchc51.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/metrics/metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/Users/yuvraj/miniconda/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 962, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 15 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast_1)' with input shapes: [?,15].\n"
     ]
    }
   ],
   "source": [
    "trained_model, training_history = fit_model(trial_model, train_dataset, steps_per_epoch=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    }
   ],
   "source": [
    "trial_model_2 = build_model(hp, max_len=15, item_vocab_size=tokenizer_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " item_id (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_item (Embedding)     (None, 15, 64)       280576      ['item_id[0][0]']                \n",
      "                                                                                                  \n",
      " batchnorm_inputs (BatchNormali  (None, 15, 64)      256         ['embedding_item[0][0]']         \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " LSTM_cat (LSTM)                (None, 15, 32)       12416       ['batchnorm_inputs[0][0]']       \n",
      "                                                                                                  \n",
      " batchnorm_lstm (BatchNormaliza  (None, 15, 32)      128         ['LSTM_cat[0][0]']               \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 15, 32)       0           ['batchnorm_lstm[0][0]',         \n",
      "                                                                  'batchnorm_lstm[0][0]']         \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 15, 4384)     144672      ['attention[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 438,048\n",
      "Trainable params: 437,856\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trial_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[388], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model, training_history \u001b[39m=\u001b[39m fit_model(trial_model, train_dataset, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[381], line 20\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model, train_dataset, steps_per_epoch, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m mirrored_strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m     18\u001b[0m     mirrored_model \u001b[39m=\u001b[39m model\n\u001b[0;32m---> 20\u001b[0m history \u001b[39m=\u001b[39m mirrored_model\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[1;32m     21\u001b[0m                              steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m     22\u001b[0m                              epochs\u001b[39m=\u001b[39;49mepochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m mirrored_model, history\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "trained_model, training_history = fit_model(trial_model, train_dataset, steps_per_epoch=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ED = keras.Sequential()\n",
    "model_ED.add(keras.layers.LSTM(150, batch_input_shape=(50, 15, 1), stateful=True))\n",
    "model_ED.add(keras.layers.RepeatVector(15))\n",
    "model_ED.add(keras.layers.LSTM(150, return_sequences=True, stateful=True))\n",
    "model_ED.add(keras.layers.TimeDistributed(keras.layers.Dense(tokenizer_vocab_size, activation='softmax')))\n",
    "model_ED.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='Adam', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = training_data_X.reshape(50000, 15, 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 15, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 132],\n",
       "       [  81],\n",
       "       [  87],\n",
       "       [  53],\n",
       "       [ 359],\n",
       "       [ 865],\n",
       "       [ 267],\n",
       "       [  22],\n",
       "       [1611],\n",
       "       [ 804],\n",
       "       [  46],\n",
       "       [  28],\n",
       "       [ 215],\n",
       "       [  11],\n",
       "       [  79]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 865],\n",
       "       [ 267],\n",
       "       [  22],\n",
       "       [1611],\n",
       "       [ 804],\n",
       "       [  46],\n",
       "       [  28],\n",
       "       [ 215],\n",
       "       [  11],\n",
       "       [  79],\n",
       "       [ 447],\n",
       "       [   5],\n",
       "       [  15],\n",
       "       [ 550],\n",
       "       [  84]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y = training_data_y.reshape(50000, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 15, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 04:02:07.315779: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 39s 38ms/step - loss: 8.3713 - sparse_categorical_accuracy: 0.0155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cd4d7400>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ED.fit(_X, _y, batch_size=50, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 132],\n",
       "        [  81],\n",
       "        [  87],\n",
       "        [  53],\n",
       "        [ 359],\n",
       "        [ 865],\n",
       "        [ 267],\n",
       "        [  22],\n",
       "        [1611],\n",
       "        [ 804],\n",
       "        [  46],\n",
       "        [  28],\n",
       "        [ 215],\n",
       "        [  11],\n",
       "        [  79]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X[0].reshape(1, 15,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 14s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model_ED.predict(_X, batch_size=50)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4384)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.3849020e-09, 2.6411006e-05, 9.9977034e-01, 3.3581380e-06,\n",
       "       6.4954620e-05], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31776491a246694c24ae6597487ab3609a52df1eaf86d1d4e4376cc81f09d2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
